{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report, r2_score\n",
    "import pickle\n",
    "\n",
    "from src.annotator_features import get_most_controversial_annotations, get_annotator_biases, get_text_entropies\n",
    "from src.train import prepare_dataloader, Classifier, predict\n",
    "from src.models import Net\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data/selected_texts/'\n",
    "\n",
    "texts_df = pd.read_csv(base_path + 'cawi2_selected_texts.csv', sep=',').iloc[:, 1:].copy()\n",
    "annotations_df = pd.read_csv(base_path + 'cawi2_selected_annotations.csv', sep=',')\n",
    "annotators_df = pd.read_csv(base_path + 'cawi2_selected_annotators.csv', sep=',')\n",
    "folds_df = pd.read_csv(base_path + 'annotator_folds.csv', sep=',')\n",
    "\n",
    "merged_annotations = texts_df.merge(annotations_df).merge(folds_df).dropna()\n",
    "merged_annotations = merged_annotations.loc[merged_annotations.annotator_id.isin(annotators_df.identyfikator)].copy()\n",
    "\n",
    "personal_df = merged_annotations[merged_annotations.split == 'past']\n",
    "\n",
    "emotion_columns = annotations_df.columns[2:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normlize_annotations(df, max_1=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    mins = df.loc[:, emotion_columns].values.min(axis=0)\n",
    "    df.loc[:, emotion_columns] = (df.loc[:, emotion_columns] - mins)\n",
    "\n",
    "    if max_1:\n",
    "        maxes = df.loc[:, emotion_columns].values.max(axis=0)\n",
    "        df.loc[:, emotion_columns] = df.loc[:, emotion_columns] / maxes\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model embeddngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings import prepare_embeddings\n",
    "prepare_embeddings()\n",
    "\n",
    "language_embeddings = {}\n",
    "languages = ['english',\n",
    " 'dutch',\n",
    " 'french',\n",
    " 'german',\n",
    " 'italian',\n",
    " 'russian',\n",
    " 'portuguese',\n",
    " 'spanish']\n",
    "\n",
    "for language in languages:\n",
    "    language_embeddings[language] = pickle.load(open(f'./data/multilingual/{language}_xlm_embeddings.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_features = annotators_df.iloc[:, 1:].fillna('empty')\n",
    "\n",
    "onehots = []\n",
    "for col in annotator_features.columns:\n",
    "    onehot = pd.get_dummies(annotator_features[col]).values\n",
    "    onehots.append(onehot)\n",
    "    \n",
    "annotator_features_onehot = np.hstack(onehots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_features_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_values_df = merged_annotations.loc[:, emotion_columns].fillna('empty')\n",
    "\n",
    "class_dims = []\n",
    "for col in annotation_values_df.columns:\n",
    "    onehot = pd.get_dummies(annotation_values_df[col]).values\n",
    "    class_dims.append(onehot.shape[1])\n",
    "\n",
    "sum(class_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset ids to enumerate from 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id_idx_dict = texts_df.loc[:, ['text_id']].reset_index().set_index('text_id').to_dict()['index']\n",
    "annotator_id_idx_dict = annotators_df.loc[:, ['identyfikator']].reset_index().set_index('identyfikator').to_dict()['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score_from_results(test_predictions, true_labels, class_dims):\n",
    "    dims_results = {}\n",
    "    for cls_idx in range(len(class_dims)):\n",
    "        start_idx =  sum(class_dims[:cls_idx])\n",
    "        end_idx =  start_idx + class_dims[cls_idx]\n",
    "        preds = torch.argmax(test_predictions[:, start_idx:end_idx], dim=1)\n",
    "\n",
    "        dims_results[cls_idx] = classification_report(true_labels[:, cls_idx].cpu(), preds.cpu(), output_dict=True)\n",
    "\n",
    "    return dims_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merged_annotations.loc[merged_annotations.split == 'present'].copy()\n",
    "dev_df = merged_annotations.loc[merged_annotations.split == 'future1'].copy()\n",
    "test_df = merged_annotations.loc[merged_annotations.split == 'future2'].copy()\n",
    "\n",
    "train_df = normlize_annotations(train_df)\n",
    "dev_df = normlize_annotations(dev_df)\n",
    "test_df = normlize_annotations(test_df)\n",
    "\n",
    "for df in [train_df, dev_df, test_df]:\n",
    "    df['text_idx'] = df['text_id'].apply(lambda w_id: text_id_idx_dict[w_id])\n",
    "    df['annotator_idx'] = df['annotator_id'].apply(lambda r_id: annotator_id_idx_dict[r_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for scenario in ['s0', 's1', 's2', 's3', 's4', 's5']:\n",
    "    results[scenario] = {}\n",
    "    for language in languages:\n",
    "        results[scenario][language] = {}\n",
    "        \n",
    "        for fold_num in range(1):\n",
    "            future1_fold_num = fold_num\n",
    "            future2_fold_num = (fold_num + 1) % 10\n",
    "\n",
    "            present_X = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), ['text_idx', 'annotator_idx']].values\n",
    "            present_y = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), emotion_columns].values\n",
    "\n",
    "            future1_X = dev_df.loc[dev_df.fold == future1_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "            future1_y = dev_df.loc[dev_df.fold == future1_fold_num, emotion_columns].values\n",
    "\n",
    "            future2_X = test_df.loc[test_df.fold == future2_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "            future2_y = test_df.loc[test_df.fold == future2_fold_num, emotion_columns].values\n",
    "\n",
    "            # biases for train datset\n",
    "            filtered_personal_df = personal_df[~personal_df.fold.isin([future1_fold_num, future2_fold_num])]\n",
    "            filtered_annotations = get_most_controversial_annotations(filtered_personal_df, emotion_columns, None)\n",
    "            annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "            annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                                .merge(annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                                .fillna(0))\n",
    "\n",
    "            # biases for test dataset\n",
    "            filtered_annotations = get_most_controversial_annotations(personal_df, emotion_columns, None)\n",
    "            test_annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "            test_annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                                .merge(test_annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                                .fillna(0))\n",
    "\n",
    "            features = language_embeddings[language], annotator_features_onehot, annotator_biases.iloc[:, 1:].values\n",
    "            test_features = language_embeddings[language], annotator_features_onehot, test_annotator_biases.iloc[:, 1:].values\n",
    "\n",
    "            if scenario == 's0':\n",
    "                s0_predictions = np.tile(present_y.mean(axis=0).round(), (future2_y.shape[0], 1))\n",
    "                results[scenario][fold_num] = ([classification_report(future2_y[:, i], s0_predictions[:, i], output_dict=True) \n",
    "                                                for i in range(future2_y.shape[1])])\n",
    "            else:\n",
    "                dataloader = prepare_dataloader(present_X, present_y, features, scenario)\n",
    "                text_feature_num = next(iter(dataloader))[0].size(-1)\n",
    "                additional_feature_num = next(iter(dataloader))[1].size(-1)\n",
    "\n",
    "                classes_num = sum(class_dims)\n",
    "                model = Net(classes_num, text_feature_num, additional_feature_num).to(device)\n",
    "                classifer = Classifier(model=model, output_type='onehot', output_dims=class_dims).to(device)\n",
    "\n",
    "                test_predictions, true_labels = predict(classifer,\n",
    "                                                        present_X, \n",
    "                                                        future1_X, \n",
    "                                                        future2_X, \n",
    "                                                        present_y, \n",
    "                                                        future1_y, \n",
    "                                                        future2_y, \n",
    "                                                        features,\n",
    "                                                        test_features,\n",
    "                                                        scenario,\n",
    "                                                        epochs=2)\n",
    "\n",
    "                results[scenario][language][fold_num] = get_f1_score_from_results(test_predictions, true_labels, class_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tuples = []\n",
    "for scenario in results.keys():\n",
    "    for language in results[scenario].keys():\n",
    "        for fold_num in results[scenario][language].keys():\n",
    "            \n",
    "            f1_result = np.mean([results[scenario][language][fold_num][i]['macro avg']['f1-score'] for i in range(10)])\n",
    "            result_tuples.append((scenario, language, fold_num, f1_result))\n",
    "            \n",
    "results_df = pd.DataFrame(result_tuples)\n",
    "results_df.columns = ['scenario', 'language', 'fold_num', 'macro f1']\n",
    "results_df = results_df.groupby(['scenario', 'language'])['macro f1'].mean().reset_index()\n",
    "\n",
    "results_df = results_df.pivot(index='language', columns='scenario', values='macro f1')\n",
    "results_df.columns = ['s0 (AVG)', 's1 (TXT)', 's3 (PEB)', 's2 (TXT+DEM)', 's4 (TXT+PEB)', 's5 (TXT+PEB+DEM)']\n",
    "results_df = results_df.reindex(sorted(results_df.columns), axis=1)\n",
    "\n",
    "print('Macro F1')\n",
    "results_df *= 100\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_score_from_results(test_predictions, true_labels):\n",
    "    true_labels = true_labels.cpu().numpy()#[:, i]\n",
    "    test_predictions = test_predictions.cpu().numpy()#[:, i]\n",
    "    \n",
    "    losses = [r2_score(true_labels[:, i], test_predictions[:, i]) for i in range(test_predictions.shape[1])]\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merged_annotations.loc[merged_annotations.split == 'present'].copy()\n",
    "dev_df = merged_annotations.loc[merged_annotations.split == 'future1'].copy()\n",
    "test_df = merged_annotations.loc[merged_annotations.split == 'future2'].copy()\n",
    "\n",
    "train_df = normlize_annotations(train_df, True)\n",
    "dev_df = normlize_annotations(dev_df, True)\n",
    "test_df = normlize_annotations(test_df, True)\n",
    "\n",
    "for df in [train_df, dev_df, test_df]:\n",
    "    df['text_idx'] = df['text_id'].apply(lambda w_id: text_id_idx_dict[w_id])\n",
    "    df['annotator_idx'] = df['annotator_id'].apply(lambda r_id: annotator_id_idx_dict[r_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for scenario in ['s0', 's1', 's2', 's3', 's4', 's5']:\n",
    "    results[scenario] = {}\n",
    "    for language in languages:\n",
    "        results[scenario][language] = {}\n",
    "        for fold_num in range(10):\n",
    "            future1_fold_num = fold_num\n",
    "            future2_fold_num = (fold_num + 1) % 10\n",
    "\n",
    "            present_X = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), ['text_idx', 'annotator_idx']].values\n",
    "            present_y = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), emotion_columns].values\n",
    "\n",
    "            future1_X = dev_df.loc[dev_df.fold == future1_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "            future1_y = dev_df.loc[dev_df.fold == future1_fold_num, emotion_columns].values\n",
    "\n",
    "            future2_X = test_df.loc[test_df.fold == future2_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "            future2_y = test_df.loc[test_df.fold == future2_fold_num, emotion_columns].values\n",
    "\n",
    "            filtered_personal_df = personal_df[~personal_df.fold.isin([future1_fold_num, future2_fold_num])]\n",
    "            filtered_annotations = get_most_controversial_annotations(filtered_personal_df, emotion_columns, None)\n",
    "            annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "            annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                                .merge(annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                                .fillna(0))\n",
    "\n",
    "            filtered_annotations = get_most_controversial_annotations(personal_df, emotion_columns, None)\n",
    "            test_annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "            test_annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                                .merge(test_annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                                .fillna(0))\n",
    "\n",
    "            features = language_embeddings[language], annotator_features_onehot, annotator_biases.iloc[:, 1:].values\n",
    "            test_features = language_embeddings[language], annotator_features_onehot, test_annotator_biases.iloc[:, 1:].values\n",
    "\n",
    "            if scenario == 's0':\n",
    "                s0_predictions = torch.tensor(np.tile(present_y.mean(axis=0), (future2_y.shape[0], 1)))\n",
    "                results[scenario][fold_num] = np.array([r2_score(future2_y[:, i], s0_predictions[:, i]) \n",
    "                                                        for i in range(future2_y.shape[1])])\n",
    "\n",
    "            else:\n",
    "                dataloader = prepare_dataloader(present_X, present_y, features, scenario)\n",
    "                text_feature_num = next(iter(dataloader))[0].size(-1)\n",
    "                additional_feature_num = next(iter(dataloader))[1].size(-1)\n",
    "\n",
    "                classes_num = 10\n",
    "                model = Net(classes_num, text_feature_num, additional_feature_num).to(device)\n",
    "                classifer = Classifier(model=model, output_type='mse', output_dims=None).to(device)\n",
    "\n",
    "                test_predictions, true_labels = predict(classifer,\n",
    "                                                        present_X, \n",
    "                                                        future1_X, \n",
    "                                                        future2_X, \n",
    "                                                        present_y, \n",
    "                                                        future1_y, \n",
    "                                                        future2_y, \n",
    "                                                        features,\n",
    "                                                        test_features,\n",
    "                                                        scenario,\n",
    "                                                        epochs=15)\n",
    "\n",
    "                results[scenario][language][fold_num] = get_r2_score_from_results(test_predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tuples = []\n",
    "for scenario in results.keys():\n",
    "    for language in results[scenario].keys():\n",
    "        for fold_num in results[scenario][language].keys():\n",
    "            \n",
    "            f1_result = np.mean([results[scenario][language][fold_num][i] for i in range(10)])\n",
    "            result_tuples.append((scenario, language, fold_num, f1_result))\n",
    "            \n",
    "results_df = pd.DataFrame(result_tuples)\n",
    "results_df.columns = ['scenario', 'language', 'fold_num', 'r^2']\n",
    "results_df = results_df.groupby(['scenario', 'language'])['r^2'].mean().reset_index()\n",
    "\n",
    "results_df = results_df.pivot(index='language', columns='scenario', values='r^2')\n",
    "results_df.columns = ['s0', 's1 (TXT)', 's3 (PEB)', 's2 (TXT+DEM)', 's4 (TXT+PEB)', 's5 (TXT+PEB+DEM)']\n",
    "results_df = results_df.reindex(sorted(results_df.columns), axis=1)\n",
    "\n",
    "print('R score')\n",
    "results_df *= 100\n",
    "results_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
