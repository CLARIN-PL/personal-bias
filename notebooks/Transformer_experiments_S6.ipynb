{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report, r2_score\n",
    "import pickle\n",
    "\n",
    "from src.annotator_features import get_most_controversial_annotations, get_annotator_biases, get_text_entropies\n",
    "from src.train import prepare_dataloader, Classifier, predict\n",
    "from src.models import Net\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data/selected_texts/'\n",
    "\n",
    "texts_df = pd.read_csv(base_path + 'cawi2_selected_texts.csv', sep=',').iloc[:, 1:].copy()\n",
    "annotations_df = pd.read_csv(base_path + 'cawi2_selected_annotations.csv', sep=',')\n",
    "annotators_df = pd.read_csv(base_path + 'cawi2_selected_annotators.csv', sep=',')\n",
    "folds_df = pd.read_csv(base_path + 'annotator_folds.csv', sep=',')\n",
    "\n",
    "merged_annotations = texts_df.merge(annotations_df).merge(folds_df).dropna()\n",
    "#merged_annotations = merged_annotations.loc[merged_annotations.annotator_id.isin(annotators_df.identyfikator)].copy()\n",
    "\n",
    "personal_df = merged_annotations[merged_annotations.split == 'past']\n",
    "\n",
    "emotion_columns = annotations_df.columns[2:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>384622968</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129936705</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>987741290</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>662287953</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>988028021</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>200703762</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>988443670</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>988446860</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4708</th>\n",
       "      <td>457756720</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>634162469</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4710 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      annotator_id  fold\n",
       "0        384622968     8\n",
       "1        129936705     7\n",
       "2        987741290     8\n",
       "3        662287953     5\n",
       "4        988028021     4\n",
       "...            ...   ...\n",
       "4705     200703762     3\n",
       "4706     988443670     8\n",
       "4707     988446860     3\n",
       "4708     457756720     9\n",
       "4709     634162469     5\n",
       "\n",
       "[4710 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normlize_annotations(df, max_1=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    mins = df.loc[:, emotion_columns].values.min(axis=0)\n",
    "    df.loc[:, emotion_columns] = (df.loc[:, emotion_columns] - mins)\n",
    "\n",
    "    if max_1:\n",
    "        maxes = df.loc[:, emotion_columns].values.max(axis=0)\n",
    "        df.loc[:, emotion_columns] = df.loc[:, emotion_columns] / maxes\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model embeddngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings import prepare_embeddings\n",
    "\n",
    "prepare_embeddings()\n",
    "\n",
    "MODEL_NAME = 'herbert'\n",
    "#MODEL_NAME = 'xlmr'\n",
    "#MODEL_NAME = 'polish_roberta'\n",
    "\n",
    "all_embeddings = pickle.load(open(f'./data/{MODEL_NAME}_embeddings.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_features = annotators_df.iloc[:, 1:].fillna('empty')\n",
    "\n",
    "onehots = []\n",
    "for col in annotator_features.columns:\n",
    "    onehot = pd.get_dummies(annotator_features[col]).values\n",
    "    onehots.append(onehot)\n",
    "    \n",
    "annotator_features_onehot = np.hstack(onehots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8853, 232)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotator_features_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_values_df = merged_annotations.loc[:, emotion_columns].fillna('empty')\n",
    "\n",
    "class_dims = []\n",
    "for col in annotation_values_df.columns:\n",
    "    onehot = pd.get_dummies(annotation_values_df[col]).values\n",
    "    class_dims.append(onehot.shape[1])\n",
    "\n",
    "sum(class_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset ids to enumerate from 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id_idx_dict = texts_df.loc[:, ['text_id']].reset_index().set_index('text_id').to_dict()['index']\n",
    "annotator_id_idx_dict = annotators_df.loc[:, ['identyfikator']].reset_index().set_index('identyfikator').to_dict()['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.annotator_features import get_most_controversial_annotations, get_annotator_biases, get_random_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_score_from_results(test_predictions, true_labels):\n",
    "    true_labels = true_labels.cpu().numpy()#[:, i]\n",
    "    test_predictions = test_predictions.cpu().numpy()#[:, i]\n",
    "    \n",
    "    losses = [r2_score(true_labels[:, i], test_predictions[:, i]) for i in range(test_predictions.shape[1])]\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merged_annotations.loc[merged_annotations.split == 'present'].copy()\n",
    "dev_df = merged_annotations.loc[merged_annotations.split == 'future1'].copy()\n",
    "test_df = merged_annotations.loc[merged_annotations.split == 'future2'].copy()\n",
    "\n",
    "train_df = normlize_annotations(train_df, True)\n",
    "dev_df = normlize_annotations(dev_df, True)\n",
    "test_df = normlize_annotations(test_df, True)\n",
    "\n",
    "for df in [train_df, dev_df, test_df]:\n",
    "    df['text_idx'] = df['text_id'].apply(lambda w_id: text_id_idx_dict[w_id])\n",
    "    df['annotator_idx'] = df['annotator_id'].apply(lambda r_id: annotator_id_idx_dict[r_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for scenario in ['s3']:\n",
    "    results[scenario] = {}\n",
    "    for annotations_ordering in ['random', 'std']:\n",
    "        results[scenario][annotations_ordering] = {}\n",
    "        for num_annotations in range(15):\n",
    "            results[scenario][annotations_ordering][num_annotations] = {}\n",
    "            \n",
    "            if annotations_ordering == 'std':\n",
    "                filtered_annotations = get_most_controversial_annotations(personal_df, emotion_columns, num_annotations)\n",
    "            else:\n",
    "                filtered_annotations = get_random_annotations(personal_df, num_annotations)\n",
    "                \n",
    "            annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "            annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                                .merge(annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                                .fillna(0))\n",
    "\n",
    "            for fold_num in range(10):\n",
    "                future1_fold_num = fold_num\n",
    "                future2_fold_num = (fold_num + 1) % 10\n",
    "\n",
    "                present_X = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), ['text_idx', 'annotator_idx']].values\n",
    "                present_y = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), emotion_columns].values\n",
    "\n",
    "                future1_X = dev_df.loc[dev_df.fold == future1_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "                future1_y = dev_df.loc[dev_df.fold == future1_fold_num, emotion_columns].values\n",
    "\n",
    "                future2_X = test_df.loc[test_df.fold == future2_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "                future2_y = test_df.loc[test_df.fold == future2_fold_num, emotion_columns].values\n",
    "\n",
    "                filtered_personal_df = personal_df[~personal_df.fold.isin([future1_fold_num, future2_fold_num])]\n",
    "                filtered_annotations = get_most_controversial_annotations(filtered_personal_df, emotion_columns, None)\n",
    "                annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "                annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                                    .merge(annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                                    .fillna(0))\n",
    "\n",
    "                filtered_annotations = get_most_controversial_annotations(personal_df, emotion_columns, None)\n",
    "                test_annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "                test_annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                                    .merge(test_annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                                    .fillna(0))\n",
    "\n",
    "                features = all_embeddings, annotator_features_onehot, annotator_biases.iloc[:, 1:].values\n",
    "                test_features = all_embeddings, annotator_features_onehot, test_annotator_biases.iloc[:, 1:].values\n",
    "\n",
    "\n",
    "                dataloader = prepare_dataloader(present_X, present_y, features, scenario)\n",
    "                text_feature_num = next(iter(dataloader))[0].size(-1)\n",
    "                additional_feature_num = next(iter(dataloader))[1].size(-1)\n",
    "\n",
    "                classes_num = 10\n",
    "                model = Net(classes_num, text_feature_num, additional_feature_num).to(device)\n",
    "                classifer = Classifier(model=model, output_type='mse', output_dims=None).to(device)\n",
    "\n",
    "                test_predictions, true_labels = predict(classifer,\n",
    "                                                        present_X, \n",
    "                                                        future1_X, \n",
    "                                                        future2_X, \n",
    "                                                        present_y, \n",
    "                                                        future1_y, \n",
    "                                                        future2_y, \n",
    "                                                        features,\n",
    "                                                        test_features,\n",
    "                                                        scenario,\n",
    "                                                        epochs=15)\n",
    "\n",
    "                results[scenario][annotations_ordering][num_annotations][fold_num] = get_r2_score_from_results(test_predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tuples = []\n",
    "for scenario in results.keys():\n",
    "    for sorting in results[scenario].keys():\n",
    "        for num_annotations in results[scenario][sorting].keys():\n",
    "            for fold_num in results[scenario][sorting][num_annotations].keys():\n",
    "                result_tuples.append((\n",
    "                    scenario,\n",
    "                    sorting,\n",
    "                    num_annotations,\n",
    "                    fold_num,\n",
    "                    np.mean(results[scenario][sorting][num_annotations][fold_num])\n",
    "                ))\n",
    "results_df = pd.DataFrame(result_tuples)\n",
    "results_df.columns = ['scenario', 'sorting', 'num_annotations', 'fold_num', 'R^2']\n",
    "\n",
    "results_df = results_df.groupby(['scenario', 'sorting', 'num_annotations'])['R^2'].mean().reset_index()\n",
    "\n",
    "results_df['R^2'] = results_df['R^2'] * 100\n",
    "results_df.loc[results_df.sorting=='std', 'sorting'] = r'$contr^{{std}}$'\n",
    "results_df = results_df.sort_values(by='sorting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "plt.figure(figsize=(15,10))\n",
    "g =  sns.lineplot(\n",
    "    data=results_df[results_df.num_annotations > 0],\n",
    "    x=\"num_annotations\", y=\"R^2\", hue='sorting',\n",
    "    style='sorting',\n",
    "    markers=True, dashes=False\n",
    ")\n",
    "g.set_xticks(range(15))\n",
    "\n",
    "baseline_score = results_df[results_df.num_annotations == 0]['R^2'].mean()\n",
    "\n",
    "ax = g.axes\n",
    "\n",
    "ax.axhline(baseline_score, ls='--', label='baseline')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of annotations')\n",
    "plt.ylabel('R-squared (%)')\n",
    "\n",
    "print('S5 herBERT Regression, R^2 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_emotion_columns = ['anticipation',\n",
    "   'arousal',\n",
    "   'joy',\n",
    "   'sadness',\n",
    "   'fear',\n",
    "   'disgust',\n",
    "    'surprise',\n",
    "    'trust',\n",
    "    'valence',\n",
    "    'anger',\n",
    "   ]\n",
    "\n",
    "result_tuples = []\n",
    "for scenario in results.keys():\n",
    "    for sorting in results[scenario].keys():\n",
    "        for num_annotations in results[scenario][sorting].keys():\n",
    "            for fold_num in results[scenario][sorting][num_annotations].keys():\n",
    "                for emotion_num, result in enumerate(results[scenario][sorting][num_annotations][fold_num]):\n",
    "                    result_tuples.append((\n",
    "                        scenario,\n",
    "                        sorting,\n",
    "                        num_annotations,\n",
    "                        fold_num,\n",
    "                        result,\n",
    "                        english_emotion_columns[emotion_num]\n",
    "                    ))\n",
    "results_df = pd.DataFrame(result_tuples)\n",
    "results_df.columns = ['scenario', 'sorting', 'num_annotations', 'fold_num', 'r^2', 'emotion'] \n",
    "results_df['r^2'] = results_df['r^2'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "g =  sns.lineplot(\n",
    "    data=results_df[(results_df.num_annotations > 0) & (results_df.sorting =='random')],\n",
    "    x=\"num_annotations\", y=\"r^2\", hue='emotion', ci=None,\n",
    "    style='emotion',\n",
    "    markers=True, dashes=False\n",
    ")\n",
    "g.set_xticks(range(15))\n",
    "\n",
    "\n",
    "ax = g.axes\n",
    "\n",
    "for idx, col in enumerate(english_emotion_columns):\n",
    "    baseline_score = results_df[(results_df.num_annotations == 0) & (results_df.emotion == col)]['r^2'].mean()\n",
    "\n",
    "    ax.axhline(baseline_score, ls='--', label= ' ', color=sns.color_palette()[idx])\n",
    "\n",
    "plt.legend(ncol=2)\n",
    "plt.xlabel('Number of annotations')\n",
    "plt.ylabel('R-squared (%)')\n",
    "\n",
    "print('S5 herBERT Regression, R^2 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
