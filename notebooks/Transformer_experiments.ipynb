{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report, r2_score\n",
    "import pickle\n",
    "\n",
    "from src.annotator_features import get_most_controversial_annotations, get_annotator_biases, get_text_entropies\n",
    "from src.train import prepare_dataloader, Classifier, predict\n",
    "from src.models import Net\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data/selected_texts/'\n",
    "\n",
    "texts_df = pd.read_csv(base_path + 'cawi2_selected_texts.csv', sep=',').iloc[:, 1:].copy()\n",
    "annotations_df = pd.read_csv(base_path + 'cawi2_selected_annotations.csv', sep=',')\n",
    "annotators_df = pd.read_csv(base_path + 'cawi2_selected_annotators.csv', sep=',')\n",
    "folds_df = pd.read_csv(base_path + 'annotator_folds.csv', sep=',')\n",
    "\n",
    "merged_annotations = texts_df.merge(annotations_df).merge(folds_df).dropna()\n",
    "merged_annotations = merged_annotations.loc[merged_annotations.annotator_id.isin(annotators_df.identyfikator)].copy()\n",
    "\n",
    "personal_df = merged_annotations[merged_annotations.split == 'past']\n",
    "\n",
    "emotion_columns = annotations_df.columns[2:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normlize_annotations(df, max_1=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    mins = df.loc[:, emotion_columns].values.min(axis=0)\n",
    "    df.loc[:, emotion_columns] = (df.loc[:, emotion_columns] - mins)\n",
    "\n",
    "    if max_1:\n",
    "        maxes = df.loc[:, emotion_columns].values.max(axis=0)\n",
    "        df.loc[:, emotion_columns] = df.loc[:, emotion_columns] / maxes\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model embeddngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings import prepare_embeddings\n",
    "\n",
    "prepare_embeddings()\n",
    "\n",
    "MODEL_NAME = 'herbert'\n",
    "#MODEL_NAME = 'xlmr'\n",
    "#MODEL_NAME = 'polish_roberta'\n",
    "\n",
    "all_embeddings = pickle.load(open(f'./data/{MODEL_NAME}_embeddings.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_features = annotators_df.iloc[:, 1:].fillna('empty')\n",
    "\n",
    "onehots = []\n",
    "for col in annotator_features.columns:\n",
    "    onehot = pd.get_dummies(annotator_features[col]).values\n",
    "    onehots.append(onehot)\n",
    "    \n",
    "annotator_features_onehot = np.hstack(onehots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_features_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_values_df = merged_annotations.loc[:, emotion_columns].fillna('empty')\n",
    "\n",
    "class_dims = []\n",
    "for col in annotation_values_df.columns:\n",
    "    onehot = pd.get_dummies(annotation_values_df[col]).values\n",
    "    class_dims.append(onehot.shape[1])\n",
    "\n",
    "sum(class_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset ids to enumerate from 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id_idx_dict = texts_df.loc[:, ['text_id']].reset_index().set_index('text_id').to_dict()['index']\n",
    "annotator_id_idx_dict = annotators_df.loc[:, ['identyfikator']].reset_index().set_index('identyfikator').to_dict()['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score_from_results(test_predictions, true_labels, class_dims):\n",
    "    dims_results = {}\n",
    "    for cls_idx in range(len(class_dims)):\n",
    "        start_idx =  sum(class_dims[:cls_idx])\n",
    "        end_idx =  start_idx + class_dims[cls_idx]\n",
    "        preds = torch.argmax(test_predictions[:, start_idx:end_idx], dim=1)\n",
    "\n",
    "        dims_results[cls_idx] = classification_report(true_labels[:, cls_idx].cpu(), preds.cpu(), output_dict=True)\n",
    "\n",
    "    return dims_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merged_annotations.loc[merged_annotations.split == 'present'].copy()\n",
    "dev_df = merged_annotations.loc[merged_annotations.split == 'future1'].copy()\n",
    "test_df = merged_annotations.loc[merged_annotations.split == 'future2'].copy()\n",
    "\n",
    "train_df = normlize_annotations(train_df)\n",
    "dev_df = normlize_annotations(dev_df)\n",
    "test_df = normlize_annotations(test_df)\n",
    "\n",
    "for df in [train_df, dev_df, test_df]:\n",
    "    df['text_idx'] = df['text_id'].apply(lambda w_id: text_id_idx_dict[w_id])\n",
    "    df['annotator_idx'] = df['annotator_id'].apply(lambda r_id: annotator_id_idx_dict[r_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for scenario in ['s0', 's1', 's2', 's3', 's4', 's5']:\n",
    "    results[scenario] = {}\n",
    "\n",
    "    print('scenario', scenario)\n",
    "    for fold_num in range(10):\n",
    "        future1_fold_num = fold_num\n",
    "        future2_fold_num = (fold_num + 1) % 10\n",
    "\n",
    "        present_X = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), ['text_idx', 'annotator_idx']].values\n",
    "        present_y = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), emotion_columns].values\n",
    "\n",
    "        future1_X = dev_df.loc[dev_df.fold == future1_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "        future1_y = dev_df.loc[dev_df.fold == future1_fold_num, emotion_columns].values\n",
    "\n",
    "        future2_X = test_df.loc[test_df.fold == future2_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "        future2_y = test_df.loc[test_df.fold == future2_fold_num, emotion_columns].values\n",
    "        \n",
    "        # biases for train datset\n",
    "        filtered_personal_df = personal_df[~personal_df.fold.isin([future1_fold_num, future2_fold_num])]\n",
    "        filtered_annotations = get_most_controversial_annotations(filtered_personal_df, emotion_columns, None)\n",
    "        annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "        annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                            .merge(annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                            .fillna(0))\n",
    "        \n",
    "        # biases for test dataset\n",
    "        filtered_annotations = get_most_controversial_annotations(personal_df, emotion_columns, None)\n",
    "        test_annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "        test_annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                            .merge(test_annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                            .fillna(0))\n",
    "        \n",
    "        features = all_embeddings, annotator_features_onehot, annotator_biases.iloc[:, 1:].values\n",
    "        test_features = all_embeddings, annotator_features_onehot, test_annotator_biases.iloc[:, 1:].values\n",
    "        \n",
    "        if scenario == 's0':\n",
    "            s0_predictions = np.tile(present_y.mean(axis=0).round(), (future2_y.shape[0], 1))\n",
    "            results[scenario][fold_num] = ([classification_report(future2_y[:, i], s0_predictions[:, i], output_dict=True) \n",
    "                                            for i in range(future2_y.shape[1])])\n",
    "        else:\n",
    "            dataloader = prepare_dataloader(present_X, present_y, features, scenario)\n",
    "            text_feature_num = next(iter(dataloader))[0].size(-1)\n",
    "            additional_feature_num = next(iter(dataloader))[1].size(-1)\n",
    "\n",
    "            classes_num = sum(class_dims)\n",
    "            model = Net(classes_num, text_feature_num, additional_feature_num).to(device)\n",
    "            classifer = Classifier(model=model, output_type='onehot', output_dims=class_dims).to(device)\n",
    "\n",
    "            test_predictions, true_labels = predict(classifer,\n",
    "                                                    present_X, \n",
    "                                                    future1_X, \n",
    "                                                    future2_X, \n",
    "                                                    present_y, \n",
    "                                                    future1_y, \n",
    "                                                    future2_y, \n",
    "                                                    features,\n",
    "                                                    test_features,\n",
    "                                                    scenario,\n",
    "                                                    epochs=15)\n",
    "            \n",
    "            results[scenario][fold_num] = get_f1_score_from_results(test_predictions, true_labels, class_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {k: np.mean([[results[k][f_num][c_idx]['macro avg']['f1-score'] for f_num in results[k].keys()] for c_idx in range(10)], axis=1) for k in results.keys()}\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "results_df.index = emotion_columns\n",
    "print(f'{MODEL_NAME} + OneHot classification, R^2 score')\n",
    "\n",
    "results_df.columns = ['s0 (AVG)', 's1 (TXT)', 's2 (TXT+DEM)', 's3 (PEB)', 's4 (TXT+PEB)', 's5 (TXT+PEB+DEM)']\n",
    "results_df.index = ['anticipation',\n",
    "                   'arousal',\n",
    "                   'joy',\n",
    "                   'sadness',\n",
    "                   'fear',\n",
    "                   'disgust',\n",
    "                    'surprise',\n",
    "                    'trust',\n",
    "                    'polarity',\n",
    "                    'anger',\n",
    "                   ]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mean = pd.DataFrame(results_df.values.mean(axis=0)[None,:])\n",
    "results_df_mean.columns = results_df.columns\n",
    "results_df_mean.index = [f'{MODEL_NAME} F1 score']\n",
    "results_df_mean = results_df_mean * 100\n",
    "results_df_mean = results_df_mean.round(2)\n",
    "results_df_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_score_from_results(test_predictions, true_labels):\n",
    "    true_labels = true_labels.cpu().numpy()#[:, i]\n",
    "    test_predictions = test_predictions.cpu().numpy()#[:, i]\n",
    "    \n",
    "    losses = [r2_score(true_labels[:, i], test_predictions[:, i]) for i in range(test_predictions.shape[1])]\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merged_annotations.loc[merged_annotations.split == 'present'].copy()\n",
    "dev_df = merged_annotations.loc[merged_annotations.split == 'future1'].copy()\n",
    "test_df = merged_annotations.loc[merged_annotations.split == 'future2'].copy()\n",
    "\n",
    "train_df = normlize_annotations(train_df, True)\n",
    "dev_df = normlize_annotations(dev_df, True)\n",
    "test_df = normlize_annotations(test_df, True)\n",
    "\n",
    "for df in [train_df, dev_df, test_df]:\n",
    "    df['text_idx'] = df['text_id'].apply(lambda w_id: text_id_idx_dict[w_id])\n",
    "    df['annotator_idx'] = df['annotator_id'].apply(lambda r_id: annotator_id_idx_dict[r_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for scenario in ['s0', 's1', 's2', 's3', 's4', 's5']:\n",
    "    results[scenario] = {}\n",
    "\n",
    "    print('scenario', scenario)\n",
    "    for fold_num in range(10):\n",
    "        future1_fold_num = fold_num\n",
    "        future2_fold_num = (fold_num + 1) % 10\n",
    "\n",
    "        present_X = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), ['text_idx', 'annotator_idx']].values\n",
    "        present_y = train_df.loc[~train_df.fold.isin([future1_fold_num, future2_fold_num]), emotion_columns].values\n",
    "\n",
    "        future1_X = dev_df.loc[dev_df.fold == future1_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "        future1_y = dev_df.loc[dev_df.fold == future1_fold_num, emotion_columns].values\n",
    "\n",
    "        future2_X = test_df.loc[test_df.fold == future2_fold_num, ['text_idx', 'annotator_idx']].values\n",
    "        future2_y = test_df.loc[test_df.fold == future2_fold_num, emotion_columns].values\n",
    "        \n",
    "        filtered_personal_df = personal_df[~personal_df.fold.isin([future1_fold_num, future2_fold_num])]\n",
    "        filtered_annotations = get_most_controversial_annotations(filtered_personal_df, emotion_columns, None)\n",
    "        annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "        annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                            .merge(annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                            .fillna(0))\n",
    "        \n",
    "        filtered_annotations = get_most_controversial_annotations(personal_df, emotion_columns, None)\n",
    "        test_annotator_biases = get_annotator_biases(filtered_annotations, emotion_columns)\n",
    "        test_annotator_biases = (pd.DataFrame(annotators_df.loc[:, 'identyfikator'])\n",
    "                            .merge(test_annotator_biases, right_on='annotator_id', left_on='identyfikator', how='left')\n",
    "                            .fillna(0))\n",
    "        \n",
    "        features = all_embeddings, annotator_features_onehot, annotator_biases.iloc[:, 1:].values\n",
    "        test_features = all_embeddings, annotator_features_onehot, test_annotator_biases.iloc[:, 1:].values\n",
    "        \n",
    "        if scenario == 's0':\n",
    "            s0_predictions = torch.tensor(np.tile(present_y.mean(axis=0), (future2_y.shape[0], 1)))\n",
    "            results[scenario][fold_num] = np.array([r2_score(future2_y[:, i], s0_predictions[:, i]) \n",
    "                                                    for i in range(future2_y.shape[1])])\n",
    "            \n",
    "        else:\n",
    "            dataloader = prepare_dataloader(present_X, present_y, features, scenario)\n",
    "            text_feature_num = next(iter(dataloader))[0].size(-1)\n",
    "            additional_feature_num = next(iter(dataloader))[1].size(-1)\n",
    "\n",
    "            classes_num = 10\n",
    "            model = Net(classes_num, text_feature_num, additional_feature_num).to(device)\n",
    "            classifer = Classifier(model=model, output_type='mse', output_dims=None).to(device)\n",
    "\n",
    "            test_predictions, true_labels = predict(classifer,\n",
    "                                                    present_X, \n",
    "                                                    future1_X, \n",
    "                                                    future2_X, \n",
    "                                                    present_y, \n",
    "                                                    future1_y, \n",
    "                                                    future2_y, \n",
    "                                                    features,\n",
    "                                                    test_features,\n",
    "                                                    scenario,\n",
    "                                                    epochs=15)\n",
    "            \n",
    "            results[scenario][fold_num] = get_r2_score_from_results(test_predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {k: np.mean([np.array(results[k][i]) for i in results[k].keys()], axis=0) for k in results.keys()}\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "results_df.index = emotion_columns\n",
    "print(f'{MODEL_NAME} + Regression, R^2 score')\n",
    "\n",
    "results_df.columns = ['s0 (AVG)', 's1 (TXT)',  's2 (TXT+DEM)', 's3 (PEB)', 's4 (TXT+PEB)', 's5 (TXT+PEB+DEM)']\n",
    "results_df.index = ['anticipation',\n",
    "                   'arousal',\n",
    "                   'joy',\n",
    "                   'sadness',\n",
    "                   'fear',\n",
    "                   'disgust',\n",
    "                    'surprise',\n",
    "                    'trust',\n",
    "                    'polarity',\n",
    "                    'anger',\n",
    "                   ]\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mean = pd.DataFrame(results_df.values.mean(axis=0)[None,:])\n",
    "results_df_mean.columns = results_df.columns\n",
    "results_df_mean.index = [f'{MODEL_NAME} R squared']\n",
    "results_df_mean = results_df_mean * 100\n",
    "results_df_mean.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
